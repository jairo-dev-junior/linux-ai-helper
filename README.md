# Linux AI Helper

Assistente de IA para Linux desenvolvido com Electron, React e TypeScript. Este projeto utiliza o Ollama para fornecer um assistente inteligente que pode ajudar com tarefas do sistema Linux, incluindo a cria√ß√£o e execu√ß√£o de scripts.

## üìã Pr√©-requisitos

Antes de come√ßar, certifique-se de ter instalado:

- **Node.js** (vers√£o 18 ou superior)
- **npm** (geralmente vem com o Node.js)
- **Ollama** (instru√ß√µes de instala√ß√£o abaixo)

## üöÄ Instala√ß√£o

### 1. Instalar o Ollama

O Ollama √© necess√°rio para executar os modelos de linguagem localmente. Siga os passos abaixo:

#### Linux

```bash
# Instalar o Ollama usando o script oficial
curl -fsSL https://ollama.com/install.sh | sh
```

Ou, se preferir instalar manualmente:

```bash
# Baixar e instalar
curl -L https://ollama.com/download/ollama-linux-amd64 -o /usr/local/bin/ollama
chmod +x /usr/local/bin/ollama
```

#### Verificar instala√ß√£o

Ap√≥s a instala√ß√£o, verifique se o Ollama est√° funcionando:

```bash
ollama --version
```

### 2. Iniciar o servi√ßo Ollama

O Ollama precisa estar rodando em segundo plano. Inicie o servi√ßo:

```bash
ollama serve
```

**Nota:** Mantenha este terminal aberto ou execute o Ollama como um servi√ßo do sistema.

### 3. Baixar o modelo necess√°rio

O projeto est√° configurado para usar o modelo `gpt-oss:20b`. Baixe o modelo:

```bash
ollama pull gpt-oss:20b
```

**Alternativa:** Se voc√™ preferir usar um modelo diferente ou menor, voc√™ pode:

1. Baixar outro modelo (ex: `ollama pull llama3:8b`)
2. Editar o arquivo `src/AiAgent.ts` e alterar o modelo na linha 13

### 4. Clonar e instalar depend√™ncias do projeto

```bash
# Clonar o reposit√≥rio (se ainda n√£o tiver)
git clone <url-do-repositorio>
cd linux-ai-helper

# Instalar as depend√™ncias
npm install
```

## ‚ñ∂Ô∏è Executando o Projeto

### Modo de Desenvolvimento

Para executar o projeto em modo de desenvolvimento:

```bash
npm run dev
```

Este comando ir√°:
- Iniciar o servidor de desenvolvimento Vite
- Abrir a aplica√ß√£o Electron
- Habilitar o DevTools automaticamente

**Importante:** Certifique-se de que o Ollama est√° rodando (`ollama serve`) antes de executar o projeto.

### Verificar se o Ollama est√° acess√≠vel

Voc√™ pode testar se o Ollama est√° funcionando corretamente:

```bash
curl http://localhost:11434/api/tags
```

Se retornar uma lista de modelos, o Ollama est√° funcionando corretamente.

## üèóÔ∏è Build para Produ√ß√£o

Para criar um execut√°vel do projeto:

```bash
npm run build
```

Este comando ir√°:
- Compilar o TypeScript
- Fazer o build do frontend com Vite
- Criar o execut√°vel com Electron Builder

O execut√°vel ser√° gerado na pasta `dist/` (ou conforme configurado no `electron-builder.json5`).

## üîß Configura√ß√£o

### Alterar o modelo do Ollama

Se voc√™ quiser usar um modelo diferente, edite o arquivo `src/AiAgent.ts`:

```typescript
const llm = new Ollama({
  baseUrl: "http://localhost:11434",
  model: "seu-modelo-aqui", // Altere aqui
  temperature: 0,
  maxRetries: 2
})
```

### Otimizar performance do Ollama

Para melhorar a performance, voc√™ pode configurar vari√°veis de ambiente antes de iniciar o Ollama:

```bash
export OLLAMA_NUM_CTX=2048
export OLLAMA_NUM_THREAD=4
ollama serve
```

Ou adicione essas vari√°veis ao seu arquivo `~/.bashrc` ou `~/.zshrc` para torn√°-las permanentes.

## üìù Scripts Dispon√≠veis

- `npm run dev` - Inicia o projeto em modo de desenvolvimento
- `npm run build` - Cria o build de produ√ß√£o
- `npm run lint` - Executa o linter ESLint
- `npm run preview` - Visualiza o build de produ√ß√£o

## üõ†Ô∏è Tecnologias Utilizadas

- **Electron** - Framework para aplica√ß√µes desktop
- **React** - Biblioteca para interfaces de usu√°rio
- **TypeScript** - Superset do JavaScript com tipagem est√°tica
- **Vite** - Build tool e dev server
- **LangChain** - Framework para aplica√ß√µes com LLMs
- **Ollama** - Ferramenta para executar modelos de linguagem localmente

## ‚ö†Ô∏è Troubleshooting

### Ollama n√£o est√° respondendo

1. Verifique se o servi√ßo est√° rodando: `ollama serve`
2. Verifique se a porta 11434 est√° acess√≠vel: `curl http://localhost:11434/api/tags`
3. Verifique se o modelo foi baixado: `ollama list`

### Erro ao executar scripts

O projeto possui valida√ß√µes de seguran√ßa para prevenir a execu√ß√£o de comandos perigosos. Se um script n√£o executar, verifique se n√£o cont√©m comandos bloqueados.

### Problemas com depend√™ncias

Se encontrar problemas ao instalar depend√™ncias:

```bash
# Limpar cache do npm
npm cache clean --force

# Remover node_modules e reinstalar
rm -rf node_modules package-lock.json
npm install
```

## üìÑ Licen√ßa

[Adicione informa√ß√µes de licen√ßa aqui]

## ü§ù Contribuindo

[Adicione informa√ß√µes sobre como contribuir aqui]
